**Goals of the Revised Structure**

* Strengthen PNSQC’s engineering identity  
* Attract more rigorous, evidence-based submissions  
* Differentiate from STARWEST, EuroSTAR, AgileTD, CAST, InnovateQA  
* Provide clarity to authors and reviewers  
* Make Track 3 competitive in the current AI-driven conference landscape  
* Improve alignment with sponsors’ interests

## **2026 Conference Theme**

## **Quality in the Age of Autonomy**

## Software systems are becoming faster, more interconnected, and increasingly capable of acting with reduced human oversight. At the same time, teams continue to rely on proven quality practices—testing, collaboration, judgment, and governance—to manage risk and deliver value.

## **Quality in the Age of Autonomy** examines how quality practices evolve across this spectrum, from primarily human-driven teams to highly automated and AI-enabled systems. Regardless of tooling or maturity, quality remains grounded in intent, evidence, accountability, and learning.

## PNSQC 2026 welcomes submissions that share practical experience, thoughtful analysis, and transferable lessons spanning traditional testing, Agile practices, engineering rigor, leadership, and intelligent systems.

## **Who should submit:** Practitioners and engineers involved in testing, test design, reliability, automation, or overall system quality. First-time authors and practitioners without prior conference publishing experience are encouraged to submit. PNSQC values real-world experience and transferable insight over name recognition, and welcomes papers that share lessons learned, tradeoffs, and outcomes others can apply. Example titles are intended to spark ideas and illustrate the range of submissions we welcome. Papers do not need to match these titles exactly. Strong submissions focus on real experience, thoughtful analysis, and lessons others can apply.

## 

## **Track 1 — Quality Engineering & Systems Reliability**

**Engineering practices, architectures, metrics, and reliability techniques grounded in evidence and real-world experience.**

This track focuses on how teams design, test, and sustain quality in software systems of all kinds—whether systems are built in-house, assembled from multiple components, or integrated with vendor-provided platforms. Submissions may address human-driven testing, automation where appropriate, or combinations of both, with an emphasis on what actually works in practice to improve reliability and reduce risk.  
Topics may include:

* Test design, exploratory testing, and coverage strategies

* Automation approaches and long-term maintainability

* System, integration, and reliability testing

* Observability, metrics, and feedback loops

* Managing technical risk in complex or integrated systems

Example paper titles:

* Exploratory Testing as a Reliability Signal, Not a Last Resort

* What Actually Improved System Stability: Lessons from Production Defects

* Balancing Manual and Automated Testing in a Growing Product

* Testing Integrations When You Don’t Control All the Dependencies

* Coverage Isn’t Confidence: Rethinking What Our Tests Really Tell Us

* Improving Reliability Without Adding More Tests

* How We Identified Risk Early Using Lightweight Testing Techniques

* From Defects to Insights: Using Failures to Strengthen System Design

* Designing Tests for Complex Systems with Limited Observability

* When Simpler Testing Approaches Outperformed Sophisticated Tooling

## **Track 2 — Organizational Quality & Leadership** 

**Quality governance, decision frameworks, culture, and organizational strategy tied to measurable outcomes.**

This track focuses on how people and organizations plan, govern, and sustain quality—especially as delivery speeds increase, systems grow more complex, and responsibility is distributed across teams and vendors. It is a strong home for Agile practices, test management, and leadership perspectives that shape how quality decisions are made, communicated, and acted upon.

The track also welcomes perspectives from enterprise and public-sector organizations responsible for **procuring, configuring, integrating, and assuring the quality of software they do not fully control**, where visibility, accountability, and risk management are critical.

### **Topics may include:**

* Agile and Lean quality practices

* Test management, planning, and reporting

* Risk-based testing and prioritization

* Quality metrics and decision frameworks

* Leadership, culture, and cross-team coordination

* Managing quality across vendors, platforms, and integrated systems

### **Example paper titles:**

* *Test Management That Supports Teams Instead of Slowing Them Down*

* *How We Made Better Quality Decisions with Fewer Metrics*

* *Aligning Agile Teams on Quality Without Heavy Process*

* *Risk-Based Testing as an Organizational Practice, Not a Spreadsheet*

* *Turning Test Reports into Decisions Leaders Actually Use*

* *Quality Ownership in Cross-Functional Teams: What Worked and What Didn’t*

* *Planning for Quality in Fast-Moving Agile Environments*

* *Managing Quality Across Multiple Teams and Vendors*

* *Quality Governance When You Don’t Own the Code*

* *Creating a Shared Definition of Quality Across Product, Engineering, and QA*

## **Track 3 — Emerging Technologies and AI Systems**

**Testing, evaluating, governing, and assuring trustworthy AI-enabled and intelligent technologies.**

This track addresses quality challenges unique to AI-enabled and intelligent systems, including how such systems are evaluated, governed, and monitored in real-world use. Submissions should emphasize practical testing and assessment techniques, governance and oversight approaches, and lessons learned from deployment and operation—rather than speculative or promotional content.

### **Topics may include:**

* Testing AI-enabled features and systems

* Bias, fairness, explainability, and transparency

* Reliability, safety, and validation of intelligent behavior

* Human oversight and governance models

* Regulatory, ethical, and compliance considerations

### **Example paper titles:**

* *Testing AI Features When Expected Results Aren’t Deterministic*

* *What “Good Enough” Means for AI-Driven Decisions*

* *Introducing AI Testing into a Traditional QA Organization*

* *Evaluating AI Systems Without a Data Science Background*

* *Bias, Risk, and Responsibility: A Tester’s Perspective*

* *Human Oversight Models That Actually Work for AI Systems*

* *Validating Intelligent Behavior in Production Environments*

* *Lessons Learned Governing AI Features Under Real Constraints*

* *Trusting AI Outputs: What We Measure and Why*

* *When AI Behaved Unexpectedly: How We Detected, Diagnosed, and Responded*

## **Track 4 (half track) — Tools and Productivity**

## **(Satish to flesh out the track description)** 

